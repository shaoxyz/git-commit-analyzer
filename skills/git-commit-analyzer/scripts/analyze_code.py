#!/usr/bin/env python3
"""
Objective code analysis for git commits.
Detects bullshit patterns and calculates real contribution metrics.

ç‰›é©¬é‰´å®šå™¨ - å®¢è§‚åˆ†æå±‚
æ£€æµ‹åˆ· commitã€AI ç”Ÿæˆã€å¤åˆ¶ç²˜è´´ç­‰ä½è´¨é‡æ¨¡å¼
"""

import argparse
import json
import re
from dataclasses import dataclass, asdict, field
from pathlib import Path

AST_GREP_AVAILABLE = False
analyze_diff_with_ast_grep = None
get_linus_comments_for_issues = None

try:
    from ast_analyzer import (
        analyze_diff_with_ast_grep as _analyze_diff,
        get_linus_comments_for_issues as _get_linus_comments,
    )

    analyze_diff_with_ast_grep = _analyze_diff
    get_linus_comments_for_issues = _get_linus_comments
    AST_GREP_AVAILABLE = True
except ImportError:
    pass


@dataclass
class CodeMetrics:
    lines_added: int = 0
    lines_deleted: int = 0
    files_changed: int = 0
    effective_lines_added: int = 0
    effective_lines_deleted: int = 0
    functions_added: int = 0
    functions_modified: int = 0
    classes_added: int = 0
    test_lines_added: int = 0
    doc_lines_added: int = 0
    is_formatting_only: bool = False
    is_rename_only: bool = False
    is_auto_generated: bool = False
    is_likely_copypaste: bool = False
    is_trivial: bool = False
    warnings: list[str] = field(default_factory=list)
    substance_score: float = 0.0
    bullshit_score: float = 0.0
    ast_grep_issues: int = 0
    ast_grep_bullshit: float = 0.0
    code_smells: list[str] = field(default_factory=list)


# Patterns for detecting auto-generated code
AUTO_GENERATED_PATTERNS = [
    r"// Code generated .* DO NOT EDIT",
    r"# Auto-generated",
    r"# Generated by",
    r"// Generated by",
    r"<!-- Auto-generated -->",
    r"// <auto-generated>",
    r"# This file is auto-generated",
    r"# DO NOT EDIT",
    r"// DO NOT EDIT",
    r"@generated",
    r"eslint-disable",  # Often indicates generated/copied code
    r"prettier-ignore",
]

# Patterns for detecting boilerplate/scaffolding
BOILERPLATE_PATTERNS = [
    r'import .* from [\'"]react[\'"]',
    r"from django\.",
    r"from flask import",
    r"package main",
    r"public static void main",
    r'if __name__ == [\'"]__main__[\'"]',
]

# File patterns that are typically auto-generated
AUTO_GENERATED_FILES = [
    r"package-lock\.json",
    r"yarn\.lock",
    r"Cargo\.lock",
    r"poetry\.lock",
    r"\.min\.js$",
    r"\.min\.css$",
    r"\.d\.ts$",
    r"__pycache__",
    r"\.pyc$",
    r"node_modules/",
    r"vendor/",
    r"dist/",
    r"build/",
    r"\.generated\.",
    r"_generated\.",
    r"swagger.*\.json",
    r"openapi.*\.json",
]

# Test file patterns
TEST_FILE_PATTERNS = [
    r"test_.*\.py$",
    r".*_test\.py$",
    r".*\.test\.[jt]sx?$",
    r".*\.spec\.[jt]sx?$",
    r"__tests__/",
    r"tests?/",
]


def is_auto_generated_file(filepath: str) -> bool:
    """Check if file is typically auto-generated."""
    for pattern in AUTO_GENERATED_FILES:
        if re.search(pattern, filepath, re.IGNORECASE):
            return True
    return False


def is_test_file(filepath: str) -> bool:
    """Check if file is a test file."""
    for pattern in TEST_FILE_PATTERNS:
        if re.search(pattern, filepath, re.IGNORECASE):
            return True
    return False


def parse_diff(diff_text: str) -> dict:
    """Parse unified diff format into structured data."""
    files = []
    current_file = None
    added_lines = []
    deleted_lines = []

    for line in diff_text.split("\n"):
        # New file header
        if line.startswith("+++ b/") or line.startswith("+++ "):
            if current_file:
                files.append(
                    {
                        "path": current_file,
                        "added": added_lines,
                        "deleted": deleted_lines,
                    }
                )
            current_file = line.replace("+++ b/", "").replace("+++ ", "").strip()
            added_lines = []
            deleted_lines = []
        elif line.startswith("+") and not line.startswith("+++"):
            added_lines.append(line[1:])
        elif line.startswith("-") and not line.startswith("---"):
            deleted_lines.append(line[1:])

    # Don't forget last file
    if current_file:
        files.append(
            {
                "path": current_file,
                "added": added_lines,
                "deleted": deleted_lines,
            }
        )

    return {"files": files}


def is_whitespace_only(line: str) -> bool:
    """Check if line is whitespace only."""
    return len(line.strip()) == 0


def is_comment_line(line: str) -> bool:
    """Check if line is a comment (simple heuristic)."""
    stripped = line.strip()
    return (
        stripped.startswith("#")
        or stripped.startswith("//")
        or stripped.startswith("/*")
        or stripped.startswith("*")
        or stripped.startswith('"""')
        or stripped.startswith("'''")
        or stripped.startswith("<!--")
    )


def count_effective_lines(lines: list[str]) -> int:
    """Count non-whitespace, non-comment lines."""
    count = 0
    for line in lines:
        if not is_whitespace_only(line) and not is_comment_line(line):
            count += 1
    return count


def detect_auto_generated(lines: list[str]) -> bool:
    """Detect auto-generated code patterns."""
    content = "\n".join(lines[:50])  # Check first 50 lines
    for pattern in AUTO_GENERATED_PATTERNS:
        if re.search(pattern, content, re.IGNORECASE):
            return True
    return False


def detect_copypaste(lines: list[str], threshold: int = 10) -> bool:
    """
    Detect likely copy-paste by looking for repeated patterns.
    If we see many similar lines, it's likely copy-paste.
    """
    if len(lines) < threshold:
        return False

    # Normalize lines and count duplicates
    normalized = [re.sub(r"\s+", " ", line.strip()) for line in lines if line.strip()]
    if not normalized:
        return False

    # Check for high repetition
    unique = set(normalized)
    repetition_ratio = 1 - (len(unique) / len(normalized))

    return repetition_ratio > 0.5 and len(normalized) > threshold


def detect_formatting_only(added: list[str], deleted: list[str]) -> bool:
    """
    Detect if changes are formatting-only.
    Compare normalized versions of added vs deleted.
    """
    if not added or not deleted:
        return False

    # Normalize: remove all whitespace
    norm_added = set(re.sub(r"\s+", "", line) for line in added if line.strip())
    norm_deleted = set(re.sub(r"\s+", "", line) for line in deleted if line.strip())

    # If normalized versions are identical, it's just formatting
    if norm_added == norm_deleted and len(norm_added) > 0:
        return True

    # High overlap suggests mostly formatting
    if norm_added and norm_deleted:
        overlap = len(norm_added & norm_deleted) / max(
            len(norm_added), len(norm_deleted)
        )
        return overlap > 0.8

    return False


def detect_rename_only(files: list[dict]) -> bool:
    """Detect if commit is just file renames."""
    if not files:
        return False

    # All files have identical added/deleted content (just moved)
    for f in files:
        added_content = "\n".join(f.get("added", []))
        deleted_content = "\n".join(f.get("deleted", []))
        if added_content != deleted_content:
            return False

    return len(files) > 0


def analyze_commit(commit: dict) -> CodeMetrics:
    """Analyze a single commit and return objective metrics."""
    metrics = CodeMetrics()

    diff_text = commit.get("diff", "")
    stats = commit.get("stats", {})
    changed_files = commit.get("changed_files", [])
    message = commit.get("message", "").lower()

    # Basic stats from git
    metrics.lines_added = stats.get("additions", 0)
    metrics.lines_deleted = stats.get("deletions", 0)
    metrics.files_changed = stats.get("files_changed", 0)

    # Parse diff for detailed analysis
    parsed = parse_diff(diff_text)
    files = parsed.get("files", [])

    all_added = []
    all_deleted = []
    auto_gen_files = 0
    test_files = 0

    for f in files:
        filepath = f.get("path", "")
        added = f.get("added", [])
        deleted = f.get("deleted", [])

        all_added.extend(added)
        all_deleted.extend(deleted)

        # Check file type
        if is_auto_generated_file(filepath):
            auto_gen_files += 1
            continue

        if is_test_file(filepath):
            test_files += 1
            metrics.test_lines_added += count_effective_lines(added)

        # Effective lines (non-whitespace, non-comment)
        metrics.effective_lines_added += count_effective_lines(added)
        metrics.effective_lines_deleted += count_effective_lines(deleted)

    # Bullshit detection
    if files:
        # Formatting only?
        if detect_formatting_only(all_added, all_deleted):
            metrics.is_formatting_only = True
            metrics.warnings.append("ğŸ¨ Formatting-only change detected")

        # Rename only?
        if detect_rename_only(files):
            metrics.is_rename_only = True
            metrics.warnings.append("ğŸ“ Rename-only change detected")

        # Auto-generated?
        if detect_auto_generated(all_added) or auto_gen_files == len(files):
            metrics.is_auto_generated = True
            metrics.warnings.append("ğŸ¤– Auto-generated code detected")

        # Copy-paste?
        if detect_copypaste(all_added):
            metrics.is_likely_copypaste = True
            metrics.warnings.append("ğŸ“‹ Likely copy-paste detected")

    # Trivial commit detection
    trivial_patterns = [
        "typo",
        "fix typo",
        "formatting",
        "lint",
        "prettier",
        "eslint",
        "whitespace",
        "remove unused",
        "update version",
    ]
    if any(p in message for p in trivial_patterns):
        metrics.is_trivial = True
        metrics.warnings.append("ğŸ“ Trivial commit (based on message)")

    if metrics.effective_lines_added < 5 and metrics.effective_lines_deleted < 5:
        metrics.is_trivial = True
        if "ğŸ“ Trivial commit" not in str(metrics.warnings):
            metrics.warnings.append("ğŸ“ Trivial commit (< 5 effective lines)")

    if (
        AST_GREP_AVAILABLE
        and files
        and analyze_diff_with_ast_grep
        and get_linus_comments_for_issues
    ):
        ast_result = analyze_diff_with_ast_grep(files)
        metrics.ast_grep_issues = ast_result.get("total_matches", 0)
        metrics.ast_grep_bullshit = ast_result.get("total_bullshit_from_ast", 0)

        for match in ast_result.get("matches", [])[:10]:
            metrics.code_smells.append(f"{match['rule']}: {match['description']}")

        linus_comments = get_linus_comments_for_issues(ast_result.get("matches", []))
        metrics.warnings.extend(linus_comments[:3])

    metrics.substance_score = calculate_substance_score(metrics)
    metrics.bullshit_score = calculate_bullshit_score(metrics)

    return metrics


def calculate_substance_score(m: CodeMetrics) -> float:
    """
    Calculate how much "real" work this commit represents.
    0-100 scale.
    """
    if m.is_formatting_only or m.is_rename_only:
        return 5.0

    if m.is_auto_generated:
        return 2.0

    score = 0.0

    # Effective lines matter more than raw lines
    score += min(m.effective_lines_added * 0.5, 30)

    # Structural changes are valuable
    score += m.functions_added * 5
    score += m.classes_added * 8

    # Tests are valuable
    score += min(m.test_lines_added * 0.3, 15)

    # Deletions can be good (cleanup)
    score += min(m.effective_lines_deleted * 0.2, 10)

    # Penalties
    if m.is_likely_copypaste:
        score *= 0.3

    if m.is_trivial:
        score *= 0.5

    return min(score, 100.0)


def calculate_bullshit_score(m: CodeMetrics) -> float:
    """
    Calculate how much of this commit is fluff/noise.
    0-100 scale. Higher = more bullshit.
    """
    score = 0.0

    if m.is_formatting_only:
        score += 40

    if m.is_rename_only:
        score += 30

    if m.is_auto_generated:
        score += 50

    if m.is_likely_copypaste:
        score += 35

    if m.is_trivial:
        score += 20

    if m.lines_added > 0:
        effective_ratio = m.effective_lines_added / m.lines_added
        if effective_ratio < 0.3:
            score += 25 * (1 - effective_ratio)

    score += m.ast_grep_bullshit

    return min(score, 100.0)


def analyze_commits(commits: list[dict]) -> dict:
    """Analyze all commits and generate summary."""
    results = []

    for commit in commits:
        metrics = analyze_commit(commit)
        results.append(
            {
                "sha": commit.get("sha", "")[:8],
                "author": commit.get("author", {}).get("name", "Unknown"),
                "message": commit.get("message", "")[:100],
                "metrics": asdict(metrics),
            }
        )

    by_author: dict[str, dict] = {}

    for r in results:
        author = r["author"]
        m = r["metrics"]

        if author not in by_author:
            by_author[author] = {
                "commits": 0,
                "total_substance": 0.0,
                "total_bullshit": 0.0,
                "effective_lines": 0,
                "functions_added": 0,
                "warnings": [],
            }

        by_author[author]["commits"] += 1
        by_author[author]["total_substance"] += m["substance_score"]
        by_author[author]["total_bullshit"] += m["bullshit_score"]
        by_author[author]["effective_lines"] += m["effective_lines_added"]
        by_author[author]["functions_added"] += m["functions_added"]
        by_author[author]["warnings"].extend(m["warnings"])

    for data in by_author.values():
        if data["commits"] > 0:
            data["avg_substance"] = data["total_substance"] / data["commits"]
            data["avg_bullshit"] = data["total_bullshit"] / data["commits"]

    return {
        "commits": results,
        "by_author": dict(by_author),
        "summary": {
            "total_commits": len(commits),
            "total_effective_lines": sum(
                r["metrics"]["effective_lines_added"] for r in results
            ),
            "total_functions_added": sum(
                r["metrics"]["functions_added"] for r in results
            ),
            "avg_substance_score": sum(r["metrics"]["substance_score"] for r in results)
            / len(results)
            if results
            else 0,
            "avg_bullshit_score": sum(r["metrics"]["bullshit_score"] for r in results)
            / len(results)
            if results
            else 0,
        },
    }


def main():
    parser = argparse.ArgumentParser(description="Objective code analysis for commits")
    parser.add_argument(
        "commits_file", help="Path to commits.json from fetch_commits.py"
    )
    parser.add_argument(
        "--output", "-o", help="Output file (default: analysis_metrics.json)"
    )

    args = parser.parse_args()

    with open(args.commits_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    commits = data.get("commits", [])
    analysis = analyze_commits(commits)

    # Add metadata
    analysis["source"] = data.get("source")
    analysis["period"] = {
        "since": data.get("since"),
        "until": data.get("until"),
    }

    output_path = args.output or args.commits_file.replace(".json", "_metrics.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)

    print(f"Analysis complete: {output_path}")
    print(f"  Commits analyzed: {len(commits)}")
    print(f"  Avg substance score: {analysis['summary']['avg_substance_score']:.1f}")
    print(f"  Avg bullshit score: {analysis['summary']['avg_bullshit_score']:.1f}")


if __name__ == "__main__":
    main()
